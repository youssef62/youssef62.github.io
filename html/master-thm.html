<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>master-thm</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="styles.css" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<p><span style="position: absolute; left:3%; top: 10%;"><a
href="homepage.html">Home</a></p>
<h1 id="master-theorem-an-intuition">Master Theorem: An Intuition</h1>
<p>If you are taking a course in algorithms, you have probably
encountered the Master Theorem. It is a tool that helps you compute the
time complexity of a divide-and-conquer algorithm of the form :</p>
<ul>
<li>Given a problem of size <span class="math inline">\(n\)</span>:
<ol type="1">
<li>Divide the problem into <span
class="math inline">\(\color{red}a\)</span> subproblems of size <span
class="math inline">\(n/\color{blue}b\)</span>.<br></li>
<li>Solve each subproblem recursively. <br></li>
<li><span style="color:green">Combine the solution of the
subproblems</span> .<br></li>
</ol></li>
</ul>
<p>More formally, if the running time of the algorithm is denoted by
<span class="math inline">\(T(n)\)</span>, then the Master Theorem
considers recurrences of the form: <span class="math display">\[T(n) =
\color{red}{a}\color{black}T(n/\color{blue}{b}\color{black}) +
\color{green}f(n)\]</span> where <span
class="math inline">\(\color{red}a\)</span> and <span
class="math inline">\(\color{blue}{b}\)</span> are constants, and <span
class="math inline">\(\color{green}f\)</span> represents the running
time of combining the solutions of the subproblems (step 3 above).</p>
You probably also remember the Master Theorem as a set of 3 rules that
you need to either memorize or look up every time you need to use it. In
this post, I will try to give you an intuition behind the Master
Theorem. The intuition is not my own, I got it from Prof. Kapralov’s
lectures on algorithms. <br> It will help you understand and remember
the theorem. But the intuition will also useful on its own, <em>to find
the time complexity of algorithms that do not fit the form of the Master
Theorem</em>. More on that later. First, let’s state the Master Theorem:
<center>
<img src="../assets/master-thm/master_thm_statement_box.png" alt="alt text" style="width: 80%;">
</center>
<p>This statement can be a bit intimidating at first. But let’s break it
down.</p>
<ul>
<li><p>We have 3 cases, each corresponding to a different relationship
between <span class="math inline">\(f(n)\)</span> and <span
class="math inline">\(n^{\log_b a}\)</span>.</p></li>
<li><p>There’s a quantity that comes up in each case: <span
class="math inline">\(n^{\log_b a}\)</span>.</p></li>
</ul>
<center>
<img src="../assets/master-thm/master-thm_statement_color.png" alt="alt text" style="width: 80%;">
</center>
<p>It is critical to understand what the quantity <span
class="math inline">\(n^{\log_b a}\)</span> represents. To understand
it, let’s start by computing the number of leaf node in the
computational tree. Each node in this tree represents a subproblem (a
function call). This tree is represented in the figure below where <span
class="math inline">\(a=2\)</span>.</p>
<center>
<img src="../assets/master-thm/rec_tree.png" alt="alt text" style="width: 70%;">
</center>
<p>The height of this tree is <span
class="math inline">\(\log_b(n)\)</span> and each node has <span
class="math inline">\(a\)</span> children. So the number of leaves is
<span class="math inline">\(a^{\log_b(n)}\)</span>. Now if we massage
this expression a bit, we get : <span class="math display">\[
\color{red}a\color{black}^{\log_b(n)} = \left( \color{red} b^{\log_b a}
\color{black} \right)^{\log_b(n)} = \left( b^{\log_b(n)} \right)^{\log_b
a} = n^{\log_b a}\]</span> where the first equality comes from the
identity <span class="math inline">\(a = b^{\log_b a}\)</span> and the
second from swapping the exponents. So <span
class="math inline">\(n^{\log_b a}\)</span> is nothing but the number of
leaves in the computational tree !</p>
<!-- Let's go back to the 3 cases of the Master theorem, starting with the first case. -->
<!-- <center> <span style="color:blue;"> If $f(n) = O(n^{\log_b a - \epsilon})$ for some constant $\epsilon > 0$, then the running time of the algorithm is $T(n) = \Theta(n^{\log_b a})$.
</span>
</center> -->
<p>Let’s observe that the first function call (<strong>the root of the
tree</strong>) takes <span class="math inline">\(f(n)\)</span> time
(<span class="math inline">\(f(n)\)</span> time for combining the
results and the rest is done by the other recursive calls). The very
last function calls (<strong>leaves</strong>) do just a constant amount
of work, so their total time would be <span
class="math inline">\(\text{\#leaves} \times \text{constant}\)</span>
i.e <span class="math inline">\(\Theta(n^{\log_b a})\)</span>. So :</p>
<ul>
<li>Root : <span class="math inline">\(f(n)\)</span> time total.</li>
<li>Leaves : <span class="math inline">\(n^{\log_b a}\)</span> time
total.</li>
</ul>
<!-- The first case assumes that $f(n) = O(n^{\log_b a - \epsilon})$, meaning that $f$ is asymptotically smaller that $n^{\log_b a}$. This intuitively means that *the work done is concentrated at the leaves*. So total runtime will be the total runtime *of the leaves*. And this is indeed the case, as the master theorem gives us that $T(n) = \Theta(n^{\log_b a})$.  -->
Provided with this, we can give an intuitive, unformal, statement of the
master theorem :
<div style="position:   relative; width: 145%;">
<table style="all: unset;width: 100%; table-layout: fixed;">
<tr >
<td style="vertical-align: top;">
<ol type="1">
<li><span style="color:blue;">If the <b>work is concentrated at the
leaves</b>, then the <b>total runtime is the total runtime of the
leaves</b>.</span><br><br>
</td>
<td style="padding: 0 5%; vertical-align: top; width: 30%;">
<b>work is concentrated at the leaves</b><br> <span
class="math inline">\(f(n)\)</span> is <span
class="math inline">\(O(n^{\log_b a - \epsilon})\)</span>, so the work
done at the root is “smaller”<a href="#fn1" class="footnote-ref"
id="fnref1" role="doc-noteref"><sup>1</sup></a> than the work done at
the leafs.<br> <b>total runtime is the total runtime of the
leaves</b><br> <span class="math inline">\(T(n) = \Theta(n^{\log_b
a})\)</span><br><br>
</td>
</tr>
<tr>
<td style="vertical-align: top;">
<ol start="2" type="1">
<li><span style="color:red;">If the <b>work at each level of the tree is
the same</b>, then the total runtime is equal to: <span
class="math display">\[\textbf{work per level} \times
\textbf{height}\]</span></span><br><br>
</td>
<td style="padding: 0 5%; vertical-align: top; width: 30%;">
<b>work at each level of the tree is the same</b><br> work at root =
work at leaf<br> <span class="math inline">\(f(n) = \Theta(n^{\log_b
a})\)</span><br> <span class="math inline">\(\textbf{work per level}
\times \textbf{height}\)</span><br> <span
class="math inline">\(n^{\log_b a} \times \log_b(n)\)</span><br><br>
</td>
</tr>
<br><br><br>
<tr>
<td style="vertical-align: top;">
<ol start="3" type="1">
<li><span style="color:green;">If the <b>work is concentrated at the
root</b>, then the <b>total runtime is the total runtime of the
root</b>.</span><br><br></li>
</ol>
</td>
<td style="padding: 0 5%; vertical-align: top; width:30%;">
<b>work is concentrated at the root</b><br> <span
class="math inline">\(f(n) = \Omega(n^{\log_b a + \epsilon})\)</span>,
so it’s “greater”<span class="math inline">\(^1\)</span> than the
leaves’ total runtime.<br> <b>total runtime is the total runtime of the
root</b><br> <span class="math inline">\(T(n) =
\Theta(f(n))\)</span><br><br>
</td>
</tr>
</table>
</div></li>
</ol></li>
</ol>
<p>Basically, each case of the master theorem is a comparison between
the work done at the root and the work done at the leaves. The three
cases correspond to when the leaves dominate, when both are equal, and
when the root dominates. And the total runtime is the total runtime of
the dominating part (or <span class="math inline">\(\text{height}\times
\text{work per level}\)</span> when they are equal).</p>
<h3
id="application-of-the-intuition-of-non-master-theorem-recurrences">Application
of the intuition of non-Master Theorem recurrences</h3>
<p>Let’s consider the following recurrence : <span
class="math display">\[T(n) = T(n/4) + T(3n/4) + n^2\]</span> This
recurrence does not fit the form of the Master Theorem. However, we can
still use the intuition we developed to have a good guess of the time
complexity, which we confirm by a proof by induction.</p>
<p>The root of the tree takes <span class="math inline">\(n^2\)</span>
time. Its children take <span
class="math inline">\((\frac{3n}{4})^2\)</span> and <span
class="math inline">\((\frac{n}{4})^2\)</span> each. So the second level
of the tree takes <span class="math inline">\(\frac{10n^2}{16}\)</span>
time. We can also check the third level. What we find is that the work
done at each level is decreasing. Meaning that <em>intuitively</em> the
work will concentrate at the root. So a good guess would be that <span
class="math inline">\(T(n) = O(f(n)) = O(n^2)\)</span>.</p>
<center>
<img src="../assets/master-thm/root_concentrated.png" alt="alt text" style="width: 70%;">
</center>
<p>Let’s prove this by induction. The base case is trivial. For the
induction step, we assume that <span class="math inline">\(T(k) \leq c
k^2\)</span> for all <span class="math inline">\(k &lt; n\)</span>. Then
we have : <span class="math display">\[T(n) = T(n/4) + T(3n/4) + n^2
\leq c \left( \frac{n}{4} \right)^2 + c \left( \frac{3n}{4} \right)^2 +
n^2 = (1+\frac{9c}{16}) n^2 \leq c n^2\]</span></p>
<p>if we choose <span class="math inline">\(c \geq 16/7\)</span>. Which
ends the proof and confirms our guess.</p>
<p>Now let’s change <span class="math inline">\(f(n)\)</span> to <span
class="math inline">\(n\)</span>. We get the recurrence : <span
class="math display">\[T(n) = T(n/4) + T(3n/4) + n\]</span> This time,
the root takes <span class="math inline">\(n\)</span> time, and its
children take <span class="math inline">\(\frac{3n}{4}\)</span> and
<span class="math inline">\(\frac{n}{4}\)</span> each. So the second
level of the tree takes <span class="math inline">\(n\)</span> time. We
can check that it will be the same for next levels. So the work is
concentrated at the leaves. A good guess would be that <span
class="math inline">\(T(n) = O(n \log n)\)</span>. Which we also can
prove by induction.</p>
<center>
<img src="../assets/master-thm/same_level.png" alt="alt text" style="width: 60%;">
</center>
</div>
<section id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>“smaller” or “greater” here are the asymptotic sense.<a
href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
