<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Master Theorem: An Intuition</title>
    <link rel="stylesheet" href="styles.css">
    <script type="text/javascript" async>
      MathJax = {
        tex: {
          inlineMath: [['$', '$']],  /* Support for inline LaTeX */
          displayMath: [['$$', '$$']]  /* Support for block LaTeX */
        },
        options: {
          renderActions: {
            addMenu: [0]  /* Disable MathJax menu */
          }
        }
      };
    </script>
    <script type="text/javascript" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
<p><span style="position: absolute; left:10%; "><a href="homepage.html">Homepage</a></p>
<h1>Master Theorem: An Intuition</h1>
<p>If you are taking a course in algorithms, you have probably encountered the Master Theorem. It is a tool that helps you compute the time complexity of a divide-and-conquer algorithm of the form : </p>
<ul>
<li>Given a problem of size $n$: <ol>
<li>Divide the problem into $\color{red}a$ subproblems of size $n/\color{blue}b$.<br></li>
<li>Solve each subproblem recursively. <br></li>
<li><span style="color:green">Combine the solution of the subproblems</span> .<br> </li>
</ol>
</li>
</ul>
<p>More formally, if the running time of the algorithm is denoted by $T(n)$, then the Master Theorem considers recurrences of the form:
$$T(n) = \color{red}{a}\color{black}T(n/\color{blue}{b}\color{black}) + \color{green}f(n)$$
where $\color{red}a$ and $\color{blue}{b}$ are constants, and $\color{green}f$ represents the running time of combining the solutions of the subproblems (step 3 above). </p>
<p>You probably also remember the Master Theorem as a set of 3 rules that you need to either memorize or look up every time you need to use it. In this post, I will try to give you an intuition behind the Master Theorem. It will help you understand and remember the theorem. But the intuition will also useful on its own, <em>to find the time complexity of algorithms that do not fit the form of the Master Theorem</em>. More on that later. I got this intuition from Prof. Michael Kapralov's lectures on algorithms at EPFL. </p>
<p>First, let's state the Master Theorem: 
<center><img src="../assets/master-thm/master_thm_statement_box.png" alt="alt text" style="width: 80%;"></center></p>
<p>This statement can be a bit intimidating at first. But let's break it down. </p>
<ul>
<li>
<p>We have 3 cases, each corresponding to a different relationship between $f(n)$ and $n^{\log_b a}$.</p>
</li>
<li>
<p>There's a quantity that comes up in each case: $n^{\log_b a}$. </p>
</li>
</ul>
<p><center><img src="../assets/master-thm/master-thm_statement_color.png" alt="alt text" style="width: 80%;"></center></p>
<p>It is critical to understand what the quantity $n^{\log_b a}$ represents. 
To understand it, let's start by computing the number of leaf node in the computational tree. Each node in this tree represents a subproblem (a function call). This tree is represented in the figure below where $a=2$. </p>
<p><center><img src="../assets/master-thm/rec_tree.png" alt="alt text" style="width: 70%;"></center></p>
<p>The height of this tree is $\log_b(n)$ and each node has $a$ children. So the number of leaves is $a^{\log_b(n)}$. Now if we massage this expression a bit, we get : 
$$ \color{red}a\color{black}^{\log_b(n)} = \left( \color{red} b^{\log_b a} \color{black} \right)^{\log_b(n)} = \left( b^{\log_b(n)} \right)^{\log_b a} = n^{\log_b a}$$
where the first equality comes from the identity $a = b^{\log_b a}$ and the second from swapping the exponents. 
So $n^{\log_b a}$ is nothing but the number of leaves in the computational tree ! </p>
<!-- Let's go back to the 3 cases of the Master theorem, starting with the first case. -->

<!-- <center> <span style="color:blue;"> If $f(n) = O(n^{\log_b a - \epsilon})$ for some constant $\epsilon > 0$, then the running time of the algorithm is $T(n) = \Theta(n^{\log_b a})$.
</span>
</center> -->

<p>Let's observe that the first function call (<strong>the root of the tree</strong>) takes $f(n)$ time ($f(n)$ time for combining the results and the rest is done by the other recursive calls). The very last function calls (<strong>leaves</strong>) do just a constant amount of work, so their total time would be $\text{#leaves} \times \text{constant}$ i.e $\Theta(n^{\log_b a})$. So : </p>
<ul>
<li>Root : $f(n)$ time total. </li>
<li>Leaves : $n^{\log_b a}$ time total. </li>
</ul>
<!-- The first case assumes that $f(n) = O(n^{\log_b a - \epsilon})$, meaning that $f$ is asymptotically smaller that $n^{\log_b a}$. This intuitively means that *the work done is concentrated at the leaves*. So total runtime will be the total runtime *of the leaves*. And this is indeed the case, as the master theorem gives us that $T(n) = \Theta(n^{\log_b a})$.  -->

<p>Provided with this, we can give an intuitive, unformal, statement of the master theorem : </p>
<div style="position: relative; width: 180%;">
  <table style="width: 100%; table-layout: fixed;">
    <tr>
      <td style="vertical-align: top;">
        1. <span style="color:blue;">If the <b>work is concentrated at the leaves</b>, then the <b>total runtime is the total runtime of the leaves</b>.</span><br><br>
      </td>
      <td style="padding: 0 10%; vertical-align: top;">
        <b>work is concentrated at the leaves</b><br>
        $f(n)$ is $O(n^{\log_b a - \epsilon})$, so the work done at the root is "smaller"$^1$ than the total runtime.<br>
        <b>total runtime is the total runtime of the leaves</b><br>
        $T(n) = \Theta(n^{\log_b a})$<br><br>
      </td>
    </tr>
    <tr>
      <td style="vertical-align: top;"> 
        2. <span style="color:red;">If the <b>work at each level of the tree is the same</b>, then the total runtime is equal to: $$\textbf{work per level} \times \textbf{height}$$</span><br><br>
      </td>
      <td style="padding: 0 10%; vertical-align: top;">
        <b>work at each level of the tree is the same</b><br>
        work at root = work at leaf<br>
        $f(n) = \Theta(n^{\log_b a})$<br> 
        $\textbf{work per level} \times \textbf{height}$<br>
        $n^{\log_b a} \times \log_b(n)$<br><br>
      </td>
    </tr>
    <br><br><br>
    <tr>
      <td style="vertical-align: top;">
        3. <span style="color:green;">If the <b>work is concentrated at the root</b>, then the <b>total runtime is the total runtime of the root</b>.</span><br><br>
      </td>
      <td style="padding: 0 10%; vertical-align: top;">
        <b>work is concentrated at the root</b><br>
        $f(n) = \Omega(n^{\log_b a + \epsilon})$, so it's "greater"$^1$ than the leaves' total runtime.<br>
        <b>total runtime is the total runtime of the root</b><br>
        $T(n) = \Theta(f(n))$<br><br>
      </td>
    </tr>
  </table>
</div>

<p>Basically, each case of the master theorem is a comparison between the work done at the root and the work done at the leaves. The three cases correspond to when the leaves dominate, when both are equal, and when the root dominates. And the total runtime is the total runtime of the dominating part (or $\text{height}\times \text{work per level}$ when they are equal).</p>
<h3>Application of the intuition of non-Master Theorem recurrences</h3>
<p>Let's consider the following recurrence : 
$$T(n) = T(n/4) + T(3n/4) + n^2$$
This recurrence does not fit the form of the Master Theorem. However, we can still use the intuition we developed to have a good guess of the time complexity, which we confirm by a proof by induction. </p>
<p>The root of the tree takes $n^2$ time. Its children take $(\frac{3n}{4})^2$ and $(\frac{n}{4})^2$ each. So the second level of the tree takes $\frac{10n^2}{16}$ time. We can also check the third level. What we find is that the work done at each level is decreasing. Meaning that <em>intuitively</em> the work will concentrate at the root. So a good guess would be that $T(n) = O(f(n)) = O(n^2)$. </p>
<p><center><img src="../assets/master-thm/root_concentrated.png" alt="alt text" style="width: 70%;"></center></p>
<p>Let's prove this by induction. The base case is trivial. For the induction step, we assume that $T(k) \leq c k^2$ for all $k &lt; n$. Then we have :
$$T(n) = T(n/4) + T(3n/4) + n^2 \leq c \left( \frac{n}{4} \right)^2 + c \left( \frac{3n}{4} \right)^2 + n^2 = (1+\frac{9c}{16}) n^2 \leq c n^2$$</p>
<p>if we choose $c \geq 16/7$. Which ends the proof and confirms our guess. </p>
<p>Now let's change $f(n)$ to $n$. We get the recurrence : 
$$T(n) = T(n/4) + T(3n/4) + n$$
This time, the root takes $n$ time, and its children take $\frac{3n}{4}$ and $\frac{n}{4}$ each. So the second level of the tree takes $n$ time. We can check that it will be the same for next levels. So the work is concentrated at the leaves. A good guess would be that $T(n) = O(n \log n)$. Which we also can prove by induction. </p>
<p><center><img src="../assets/master-thm/same_level.png" alt="alt text" style="width: 60%;"></center></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>"smaller" or "greater" here are the asymptotic sense.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</body>
</html>
